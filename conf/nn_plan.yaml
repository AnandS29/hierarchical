defaults:
  - models: t
  - envs: cartpole

#EVALUATION PARAMS
plot: false

load_model: false
save_model: false
retrain_model: false
# TRAJ_MODELS
# t_opt2_no_hidden (trajectory model with no hidden layers in NN policy trained with states before done)
# t_opt2 (trajectory model trained with all states)
# t_opt2_done (trajecotry model trained with states before done)
traj_model: t_opt2_replan
use_cmaes: true
# whether or not the dynamics model is used when optimizing cmaes
no_est: false

save_video: true
save_video_est: true
dir_traj_est: est
dir_traj_vid: t_opt_vid
dir_traj_state_action: t_opt_dict
dir_model_eval_vid: model_eval_vid
dir_model_prediction: eval_model

h_width: 5
h_layers: 0

param_bounds: [-1, 1]

#number of training points to randomly sample each retrain
#set to 0 to train on all data_points
num_training_points: 0
n_iter: 10

#initial number of trajectories to train on
initial_num_trials: 100
initial_num_trial_length: 250
plan_trial_timesteps: 200
eval_trials: 1
eval_trial_length: 250
#MPC samples
num_random_configs: 10000

# number of MPC runs per iteration
# runs MPC with horizon=plan_trial_timesteps/num_MPC_per_iter
# if set to 0, will run MPC once every time step with horizon equal to value set below
# normally set to 0 or 1
num_MPC_per_iter: 200
# set this if replanning every time step
horizon: 100
replan_per_timestep: true

#only used for one-step
#use PID parameters to plan rather than random actions
action_plan: false

random_seed: 0

hydra:
  run:
    dir: ./exp/${env.name}/${now:%Y.%m.%d}/${now:%H}/${now:%M%S}
  sweep:
    dir: ./exp/${env.name}/${now:%Y.%m.%d}/${now:%H}/${now:%M%S}
    subdir: ${hydra.job.override_dirname}/${hydra.job.num}
  job:
    config:
      override_dirname:
        kv_sep: '='
        item_sep: ','
        exclude_keys: [ 'random_seed' ]
