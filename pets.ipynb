{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import omegaconf\n",
    "\n",
    "import mbrl.env.cartpole_continuous as cartpole_env\n",
    "import mbrl.env.reward_fns as reward_fns\n",
    "import mbrl.env.termination_fns as termination_fns\n",
    "import mbrl.models as models\n",
    "import mbrl.planning as planning\n",
    "import mbrl.util.common as common_util\n",
    "import mbrl.util as util\n",
    "\n",
    "import gym\n",
    "\n",
    "from history_env import *\n",
    "\n",
    "mpl.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dynamics history environment\n",
    "dt = 0.01\n",
    "t_pred = 3\n",
    "outputs = [0, 1]\n",
    "seed = 0\n",
    "\n",
    "register_history_env(\"Dubins\", t_pred, outputs, dt=dt)\n",
    "env = gym.make('DubinsEnv-v0')\n",
    "env.seed(seed)\n",
    "\n",
    "rng = np.random.default_rng(seed=0)\n",
    "generator = torch.Generator(device=device)\n",
    "generator.manual_seed(seed)\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape\n",
    "\n",
    "# This functions allows the model to evaluate the true rewards given an observation \n",
    "reward_fn = reward_fns.cartpole # TODO\n",
    "# This function allows the model to know if an observation should make the episode end\n",
    "term_fn = termination_fns.cartpole # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dynamics model\n",
    "\n",
    "trial_length = 200\n",
    "num_trials = 10\n",
    "ensemble_size = 5\n",
    "\n",
    "# Everything with \"???\" indicates an option with a missing value.\n",
    "# Our utility functions will fill in these details using the \n",
    "# environment information\n",
    "model_info = {\n",
    "    \"_target_\": \"mbrl.models.GaussianMLP\",\n",
    "    \"device\": device,\n",
    "    \"num_layers\": 3,\n",
    "    \"ensemble_size\": ensemble_size,\n",
    "    \"hid_size\": 200,\n",
    "    \"in_size\": \"???\",\n",
    "    \"out_size\": \"???\",\n",
    "    \"deterministic\": False,\n",
    "    \"propagation_method\": \"fixed_model\",\n",
    "    # can also configure activation function for GaussianMLP\n",
    "    \"activation_fn_cfg\": {\n",
    "        \"_target_\": \"torch.nn.LeakyReLU\",\n",
    "        \"negative_slope\": 0.01\n",
    "    }\n",
    "}\n",
    "# model_info[\"model\"] = model_info.copy()\n",
    "\n",
    "cfg_dict = {\n",
    "    # dynamics model configuration\n",
    "    \"dynamics_model\": model_info,\n",
    "    # options for training the dynamics model\n",
    "    \"algorithm\": {\n",
    "        \"learned_rewards\": False,\n",
    "        \"target_is_delta\": True,\n",
    "        \"normalize\": True,\n",
    "    },\n",
    "    # these are experiment specific options\n",
    "    \"overrides\": {\n",
    "        \"trial_length\": trial_length,\n",
    "        \"num_steps\": num_trials * trial_length,\n",
    "        \"model_batch_size\": 32,\n",
    "        \"validation_ratio\": 0.05\n",
    "    }\n",
    "}\n",
    "\n",
    "cfg = omegaconf.OmegaConf.create(cfg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1-D dynamics model for this environment\n",
    "dynamics_model = common_util.create_one_dim_tr_model(cfg, obs_shape, act_shape)\n",
    "\n",
    "# Create a gym-like environment to encapsulate the model\n",
    "model_env = models.ModelEnv(env, dynamics_model, term_fn, reward_fn, generator=generator)\n",
    "\n",
    "# Create replay buffer\n",
    "replay_buffer = common_util.create_replay_buffer(cfg, obs_shape, act_shape, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples stored 200\n"
     ]
    }
   ],
   "source": [
    "# Compute rollouts\n",
    "\n",
    "common_util.rollout_agent_trajectories(\n",
    "    env,\n",
    "    trial_length, # initial exploration steps\n",
    "    planning.RandomAgent(env),\n",
    "    {}, # keyword arguments to pass to agent.act()\n",
    "    replay_buffer=replay_buffer,\n",
    "    trial_length=trial_length\n",
    ")\n",
    "\n",
    "print(\"# samples stored\", replay_buffer.num_stored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup CEM agent\n",
    "\n",
    "agent_cfg = omegaconf.OmegaConf.create({\n",
    "    # this class evaluates many trajectories and picks the best one\n",
    "    \"_target_\": \"mbrl.planning.TrajectoryOptimizerAgent\",\n",
    "    \"planning_horizon\": 15,\n",
    "    \"replan_freq\": 1,\n",
    "    \"verbose\": False,\n",
    "    \"action_lb\": \"???\",\n",
    "    \"action_ub\": \"???\",\n",
    "    # this is the optimizer to generate and choose a trajectory\n",
    "    \"optimizer_cfg\": {\n",
    "        \"_target_\": \"mbrl.planning.CEMOptimizer\",\n",
    "        \"num_iterations\": 5,\n",
    "        \"elite_ratio\": 0.1,\n",
    "        \"population_size\": 500,\n",
    "        \"alpha\": 0.1,\n",
    "        \"device\": device,\n",
    "        \"lower_bound\": \"???\",\n",
    "        \"upper_bound\": \"???\",\n",
    "        \"return_mean_elites\": True,\n",
    "        \"clipped_normal\": False\n",
    "    }\n",
    "})\n",
    "\n",
    "agent = planning.create_trajectory_optim_agent_for_model(\n",
    "    model_env,\n",
    "    agent_cfg,\n",
    "    num_particles=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PETS\n",
    "\n",
    "train_losses = []\n",
    "val_scores = []\n",
    "\n",
    "def train_callback(_model, _total_calls, _epoch, tr_loss, val_score, _best_val):\n",
    "    train_losses.append(tr_loss)\n",
    "    val_scores.append(val_score.mean().item())   # this returns val score per ensemble model\n",
    "\n",
    "def update_axes(_axs, _frame, _text, _trial, _steps_trial, _all_rewards, force_update=False):\n",
    "    if not force_update and (_steps_trial % 10 != 0):\n",
    "        return\n",
    "    _axs[0].imshow(_frame)\n",
    "    _axs[0].set_xticks([])\n",
    "    _axs[0].set_yticks([])\n",
    "    _axs[1].clear()\n",
    "    _axs[1].set_xlim([0, num_trials + .1])\n",
    "    _axs[1].set_ylim([0, 200])\n",
    "    _axs[1].set_xlabel(\"Trial\")\n",
    "    _axs[1].set_ylabel(\"Trial reward\")\n",
    "    _axs[1].plot(_all_rewards, 'bs-')\n",
    "    _text.set_text(f\"Trial {_trial + 1}: {_steps_trial} steps\")\n",
    "    display.display(plt.gcf())  \n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/anandsranjan/Desktop/hybrid-workspace/mbrl-lib/hierarchical/pets.ipynb Cell 8'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anandsranjan/Desktop/hybrid-workspace/mbrl-lib/hierarchical/pets.ipynb#ch0000007?line=31'>32</a>\u001b[0m     model_trainer\u001b[39m.\u001b[39mtrain(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anandsranjan/Desktop/hybrid-workspace/mbrl-lib/hierarchical/pets.ipynb#ch0000007?line=32'>33</a>\u001b[0m         dataset_train, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anandsranjan/Desktop/hybrid-workspace/mbrl-lib/hierarchical/pets.ipynb#ch0000007?line=33'>34</a>\u001b[0m         dataset_val\u001b[39m=\u001b[39mdataset_val, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anandsranjan/Desktop/hybrid-workspace/mbrl-lib/hierarchical/pets.ipynb#ch0000007?line=34'>35</a>\u001b[0m         num_epochs\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anandsranjan/Desktop/hybrid-workspace/mbrl-lib/hierarchical/pets.ipynb#ch0000007?line=35'>36</a>\u001b[0m         patience\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anandsranjan/Desktop/hybrid-workspace/mbrl-lib/hierarchical/pets.ipynb#ch0000007?line=36'>37</a>\u001b[0m         callback\u001b[39m=\u001b[39mtrain_callback)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anandsranjan/Desktop/hybrid-workspace/mbrl-lib/hierarchical/pets.ipynb#ch0000007?line=38'>39</a>\u001b[0m \u001b[39m# --- Doing env step using the agent and adding to model dataset ---\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/anandsranjan/Desktop/hybrid-workspace/mbrl-lib/hierarchical/pets.ipynb#ch0000007?line=39'>40</a>\u001b[0m next_obs, reward, done, _ \u001b[39m=\u001b[39m common_util\u001b[39m.\u001b[39;49mstep_env_and_add_to_buffer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anandsranjan/Desktop/hybrid-workspace/mbrl-lib/hierarchical/pets.ipynb#ch0000007?line=40'>41</a>\u001b[0m     env, obs, agent, {}, replay_buffer)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anandsranjan/Desktop/hybrid-workspace/mbrl-lib/hierarchical/pets.ipynb#ch0000007?line=42'>43</a>\u001b[0m \u001b[39m# update_axes(\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anandsranjan/Desktop/hybrid-workspace/mbrl-lib/hierarchical/pets.ipynb#ch0000007?line=43'>44</a>\u001b[0m \u001b[39m#     axs, env.render(mode=\"rgb_array\"), ax_text, trial, steps_trial, all_rewards)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anandsranjan/Desktop/hybrid-workspace/mbrl-lib/hierarchical/pets.ipynb#ch0000007?line=45'>46</a>\u001b[0m obs \u001b[39m=\u001b[39m next_obs\n",
      "File \u001b[0;32m~/Desktop/hybrid-workspace/mbrl-lib/mbrl/util/common.py:599\u001b[0m, in \u001b[0;36mstep_env_and_add_to_buffer\u001b[0;34m(env, obs, agent, agent_kwargs, replay_buffer, callback, agent_uses_low_dim_obs)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    598\u001b[0m     agent_obs \u001b[39m=\u001b[39m obs\n\u001b[0;32m--> 599\u001b[0m action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mact(agent_obs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49magent_kwargs)\n\u001b[1;32m    600\u001b[0m next_obs, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m    601\u001b[0m replay_buffer\u001b[39m.\u001b[39madd(obs, action, next_obs, reward, done)\n",
      "File \u001b[0;32m~/Desktop/hybrid-workspace/mbrl-lib/mbrl/planning/trajectory_opt.py:684\u001b[0m, in \u001b[0;36mTrajectoryOptimizerAgent.act\u001b[0;34m(self, obs, optimizer_callback, **_kwargs)\u001b[0m\n\u001b[1;32m    681\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrajectory_eval_fn(obs, action_sequences)\n\u001b[1;32m    683\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 684\u001b[0m plan \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49moptimize(\n\u001b[1;32m    685\u001b[0m     trajectory_eval_fn, callback\u001b[39m=\u001b[39;49moptimizer_callback\n\u001b[1;32m    686\u001b[0m )\n\u001b[1;32m    687\u001b[0m plan_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m    689\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactions_to_use\u001b[39m.\u001b[39mextend([a \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m plan[: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplan_freq]])\n",
      "File \u001b[0;32m~/Desktop/hybrid-workspace/mbrl-lib/mbrl/planning/trajectory_opt.py:558\u001b[0m, in \u001b[0;36mTrajectoryOptimizer.optimize\u001b[0;34m(self, trajectory_eval_fn, callback)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    540\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    541\u001b[0m     trajectory_eval_fn: Callable[[torch\u001b[39m.\u001b[39mTensor], torch\u001b[39m.\u001b[39mTensor],\n\u001b[1;32m    542\u001b[0m     callback: Optional[Callable] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    543\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m    544\u001b[0m     \u001b[39m\"\"\"Runs the trajectory optimization.\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \n\u001b[1;32m    546\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[39m        (tuple of np.ndarray and float): the best action sequence.\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m     best_solution \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49moptimize(\n\u001b[1;32m    559\u001b[0m         trajectory_eval_fn,\n\u001b[1;32m    560\u001b[0m         x0\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprevious_solution,\n\u001b[1;32m    561\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    562\u001b[0m     )\n\u001b[1;32m    563\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_last_solution:\n\u001b[1;32m    564\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprevious_solution \u001b[39m=\u001b[39m best_solution\u001b[39m.\u001b[39mroll(\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplan_freq, dims\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/hybrid-workspace/mbrl-lib/mbrl/planning/trajectory_opt.py:172\u001b[0m, in \u001b[0;36mCEMOptimizer.optimize\u001b[0;34m(self, obj_fun, x0, callback, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_iterations):\n\u001b[1;32m    171\u001b[0m     population \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_population(mu, dispersion, population)\n\u001b[0;32m--> 172\u001b[0m     values \u001b[39m=\u001b[39m obj_fun(population)\n\u001b[1;32m    174\u001b[0m     \u001b[39mif\u001b[39;00m callback \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m         callback(population, values, i)\n",
      "File \u001b[0;32m~/Desktop/hybrid-workspace/mbrl-lib/mbrl/planning/trajectory_opt.py:681\u001b[0m, in \u001b[0;36mTrajectoryOptimizerAgent.act.<locals>.trajectory_eval_fn\u001b[0;34m(action_sequences)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrajectory_eval_fn\u001b[39m(action_sequences):\n\u001b[0;32m--> 681\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrajectory_eval_fn(obs, action_sequences)\n",
      "File \u001b[0;32m~/Desktop/hybrid-workspace/mbrl-lib/mbrl/planning/trajectory_opt.py:744\u001b[0m, in \u001b[0;36mcreate_trajectory_optim_agent_for_model.<locals>.trajectory_eval_fn\u001b[0;34m(initial_state, action_sequences)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrajectory_eval_fn\u001b[39m(initial_state, action_sequences):\n\u001b[0;32m--> 744\u001b[0m     \u001b[39mreturn\u001b[39;00m model_env\u001b[39m.\u001b[39;49mevaluate_action_sequences(\n\u001b[1;32m    745\u001b[0m         action_sequences, initial_state\u001b[39m=\u001b[39;49minitial_state, num_particles\u001b[39m=\u001b[39;49mnum_particles\n\u001b[1;32m    746\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/hybrid-workspace/mbrl-lib/mbrl/models/model_env.py:183\u001b[0m, in \u001b[0;36mModelEnv.evaluate_action_sequences\u001b[0;34m(self, action_sequences, initial_state, num_particles)\u001b[0m\n\u001b[1;32m    179\u001b[0m action_for_step \u001b[39m=\u001b[39m action_sequences[:, time_step, :]\n\u001b[1;32m    180\u001b[0m action_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrepeat_interleave(\n\u001b[1;32m    181\u001b[0m     action_for_step, num_particles, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m    182\u001b[0m )\n\u001b[0;32m--> 183\u001b[0m _, rewards, dones, model_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m    184\u001b[0m     action_batch, model_state, sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    185\u001b[0m )\n\u001b[1;32m    186\u001b[0m rewards[terminated] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    187\u001b[0m terminated \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m dones\n",
      "File \u001b[0;32m~/Desktop/hybrid-workspace/mbrl-lib/mbrl/models/model_env.py:118\u001b[0m, in \u001b[0;36mModelEnv.step\u001b[0;34m(self, actions, model_state, sample)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(actions, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m    112\u001b[0m     actions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(actions)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    113\u001b[0m (\n\u001b[1;32m    114\u001b[0m     next_observs,\n\u001b[1;32m    115\u001b[0m     pred_rewards,\n\u001b[1;32m    116\u001b[0m     pred_terminals,\n\u001b[1;32m    117\u001b[0m     next_model_state,\n\u001b[0;32m--> 118\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdynamics_model\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m    119\u001b[0m     actions,\n\u001b[1;32m    120\u001b[0m     model_state,\n\u001b[1;32m    121\u001b[0m     deterministic\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m sample,\n\u001b[1;32m    122\u001b[0m     rng\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rng,\n\u001b[1;32m    123\u001b[0m )\n\u001b[1;32m    124\u001b[0m rewards \u001b[39m=\u001b[39m (\n\u001b[1;32m    125\u001b[0m     pred_rewards\n\u001b[1;32m    126\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_fn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_fn(actions, next_observs)\n\u001b[1;32m    128\u001b[0m )\n\u001b[1;32m    129\u001b[0m dones \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtermination_fn(actions, next_observs)\n",
      "File \u001b[0;32m~/Desktop/hybrid-workspace/mbrl-lib/mbrl/models/one_dim_tr_model.py:278\u001b[0m, in \u001b[0;36mOneDTransitionRewardModel.sample\u001b[0;34m(self, act, model_state, deterministic, rng)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39m\"\u001b[39m\u001b[39msample_1d\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    275\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    276\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mOneDTransitionRewardModel requires wrapped model to define method sample_1d\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    277\u001b[0m     )\n\u001b[0;32m--> 278\u001b[0m preds, next_model_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49msample_1d(\n\u001b[1;32m    279\u001b[0m     model_in, model_state, rng\u001b[39m=\u001b[39;49mrng, deterministic\u001b[39m=\u001b[39;49mdeterministic\n\u001b[1;32m    280\u001b[0m )\n\u001b[1;32m    281\u001b[0m next_observs \u001b[39m=\u001b[39m preds[:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearned_rewards \u001b[39melse\u001b[39;00m preds\n\u001b[1;32m    282\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_is_delta:\n",
      "File \u001b[0;32m~/Desktop/hybrid-workspace/mbrl-lib/mbrl/models/model.py:467\u001b[0m, in \u001b[0;36mEnsemble.sample_1d\u001b[0;34m(self, model_input, model_state, deterministic, rng)\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    459\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\n\u001b[1;32m    460\u001b[0m             model_input,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    464\u001b[0m         model_state,\n\u001b[1;32m    465\u001b[0m     )\n\u001b[1;32m    466\u001b[0m \u001b[39massert\u001b[39;00m rng \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m means, logvars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\n\u001b[1;32m    468\u001b[0m     model_input, rng\u001b[39m=\u001b[39;49mrng, propagation_indices\u001b[39m=\u001b[39;49mmodel_state[\u001b[39m\"\u001b[39;49m\u001b[39mpropagation_indices\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m    469\u001b[0m )\n\u001b[1;32m    470\u001b[0m variances \u001b[39m=\u001b[39m logvars\u001b[39m.\u001b[39mexp()\n\u001b[1;32m    471\u001b[0m stds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqrt(variances)\n",
      "File \u001b[0;32m~/Desktop/hybrid-workspace/mbrl-lib/mbrl/models/gaussian_mlp.py:278\u001b[0m, in \u001b[0;36mGaussianMLP.forward\u001b[0;34m(self, x, rng, propagation_indices, use_propagation)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39m\"\"\"Computes mean and logvar predictions for the given input.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \n\u001b[1;32m    227\u001b[0m \u001b[39mWhen ``self.num_members > 1``, the model supports uncertainty propagation options\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \n\u001b[1;32m    276\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m use_propagation:\n\u001b[0;32m--> 278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_ensemble(\n\u001b[1;32m    279\u001b[0m         x, rng\u001b[39m=\u001b[39;49mrng, propagation_indices\u001b[39m=\u001b[39;49mpropagation_indices\n\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    281\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_default_forward(x)\n",
      "File \u001b[0;32m~/Desktop/hybrid-workspace/mbrl-lib/mbrl/models/gaussian_mlp.py:212\u001b[0m, in \u001b[0;36mGaussianMLP._forward_ensemble\u001b[0;34m(self, x, rng, propagation_indices)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[39mif\u001b[39;00m propagation_indices \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    210\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mWhen using propagation=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfixed_model\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, `propagation_indices` must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    211\u001b[0m         )\n\u001b[0;32m--> 212\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_from_indices(x, propagation_indices)\n\u001b[1;32m    213\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpropagation_method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mexpectation\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    214\u001b[0m     mean, logvar \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_default_forward(x, only_elite\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/hybrid-workspace/mbrl-lib/mbrl/models/gaussian_mlp.py:168\u001b[0m, in \u001b[0;36mGaussianMLP._forward_from_indices\u001b[0;34m(self, x, model_shuffle_indices)\u001b[0m\n\u001b[1;32m    161\u001b[0m num_models \u001b[39m=\u001b[39m (\n\u001b[1;32m    162\u001b[0m     \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39melite_models) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39melite_models \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[1;32m    163\u001b[0m )\n\u001b[1;32m    164\u001b[0m shuffled_x \u001b[39m=\u001b[39m x[:, model_shuffle_indices, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\u001b[39m.\u001b[39mview(\n\u001b[1;32m    165\u001b[0m     num_models, batch_size \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m num_models, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m    166\u001b[0m )\n\u001b[0;32m--> 168\u001b[0m mean, logvar \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_default_forward(shuffled_x, only_elite\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    169\u001b[0m \u001b[39m# note that mean and logvar are shuffled\u001b[39;00m\n\u001b[1;32m    170\u001b[0m mean \u001b[39m=\u001b[39m mean\u001b[39m.\u001b[39mview(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/hybrid-workspace/mbrl-lib/mbrl/models/gaussian_mlp.py:144\u001b[0m, in \u001b[0;36mGaussianMLP._default_forward\u001b[0;34m(self, x, only_elite, **_kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_default_forward\u001b[39m(\n\u001b[1;32m    141\u001b[0m     \u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor, only_elite: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs\n\u001b[1;32m    142\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, Optional[torch\u001b[39m.\u001b[39mTensor]]:\n\u001b[1;32m    143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_toggle_layers_use_only_elite(only_elite)\n\u001b[0;32m--> 144\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_layers(x)\n\u001b[1;32m    145\u001b[0m     mean_and_logvar \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean_and_logvar(x)\n\u001b[1;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_toggle_layers_use_only_elite(only_elite)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1008x270 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a trainer for the model\n",
    "model_trainer = models.ModelTrainer(dynamics_model, optim_lr=1e-3, weight_decay=5e-5)\n",
    "\n",
    "# Create visualization objects\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 3.75), gridspec_kw={\"width_ratios\": [1, 1]})\n",
    "ax_text = axs[0].text(300, 50, \"\")\n",
    "    \n",
    "# Main PETS loop\n",
    "all_rewards = [0]\n",
    "for trial in range(num_trials):\n",
    "    obs = env.reset()    \n",
    "    agent.reset()\n",
    "    \n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    steps_trial = 0\n",
    "    # update_axes(axs, env.render(mode=\"rgb_array\"), ax_text, trial, steps_trial, all_rewards)\n",
    "    while not done:\n",
    "        # --------------- Model Training -----------------\n",
    "        if steps_trial == 0:\n",
    "            dynamics_model.update_normalizer(replay_buffer.get_all())  # update normalizer stats\n",
    "            \n",
    "            dataset_train, dataset_val = common_util.get_basic_buffer_iterators(\n",
    "                replay_buffer,\n",
    "                batch_size=cfg.overrides.model_batch_size,\n",
    "                val_ratio=cfg.overrides.validation_ratio,\n",
    "                ensemble_size=ensemble_size,\n",
    "                shuffle_each_epoch=True,\n",
    "                bootstrap_permutes=False,  # build bootstrap dataset using sampling with replacement\n",
    "            )\n",
    "                \n",
    "            model_trainer.train(\n",
    "                dataset_train, \n",
    "                dataset_val=dataset_val, \n",
    "                num_epochs=50, \n",
    "                patience=50, \n",
    "                callback=train_callback,\n",
    "                silent=True)\n",
    "\n",
    "        # --- Doing env step using the agent and adding to model dataset ---\n",
    "        next_obs, reward, done, _ = common_util.step_env_and_add_to_buffer(\n",
    "            env, obs, agent, {}, replay_buffer)\n",
    "            \n",
    "        # update_axes(\n",
    "        #     axs, env.render(mode=\"rgb_array\"), ax_text, trial, steps_trial, all_rewards)\n",
    "        \n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        steps_trial += 1\n",
    "        \n",
    "        if steps_trial == trial_length:\n",
    "            break\n",
    "    \n",
    "    all_rewards.append(total_reward)\n",
    "\n",
    "# update_axes(axs, env.render(mode=\"rgb_array\"), ax_text, trial, steps_trial, all_rewards, force_update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python ('mbrl_env')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n mbrl_env ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(12, 10))\n",
    "ax[0].plot(train_losses)\n",
    "ax[0].set_xlabel(\"Total training epochs\")\n",
    "ax[0].set_ylabel(\"Training loss (avg. NLL)\")\n",
    "ax[1].plot(val_scores)\n",
    "ax[1].set_xlabel(\"Total training epochs\")\n",
    "ax[1].set_ylabel(\"Validation score (avg. MSE)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
